## 1 Software Engineering for AI/AS (AI Engineering): 

My research focuses on investigating data: how we can use it, sample it, and generate it. A key part of my research is identifying data shifts, specifically semantic and domain shifts. Currently, I am working on a project where I use generative models to create unseen data samples and rank them based on their importance. My research spans multiple fields, including generative models, hard mining, data synthesis, and outlier detection. It is industry-focused and combines various disciplines and techniques.

With a background in software engineering, I apply software engineering principles to ensure my research is easy to work with and maintainable. I emphasize fast iteration, reproducibility, and the ability to mass deploy and reuse components, whether they are third-party or self-designed.

AI development is closely aligned with software development. Regardless of whether I am creating a physics-inspired simulator or using a generator, it is still a system that I need to verify and validate to ensure it behaves as expected and solves the correct problem.

At a high level, I work with data generators, which can be deep learning models or physics-based rendering engines. I want the downstream task to guide the data generation that is needed to achieve the best possible performance.

Some challenges I face include the desire to be technology-agnostic, meaning all my components should share an interface even if they use completely different technologies underneath. This is challenging both from a software engineering perspective and from a research perspective, as I need to ensure that all components work seamlessly together. Additionally, I aim to generate pipelines that can be automatically evaluated based on my benchmarks, ensuring no bias is introduced in the verification and validation process. I need to verify that my sampling strategies behave as expected and that the data I generate is desirable. Furthermore, I must validate that the strategy actually improves the performance of the downstream task.

## 2 Robert's Lectures to My Research
In my research several principles from Robert's lectures are highly relevant. One of the key takeaways from Robert's lectures is the concept of Verification and Validation. Before these lectures, my research was primarily focused on achieving good results on benchmarks. However, we often found discrepancies between our benchmark results and performance in industrial settings. This realization led us to formalize our research approach, emphasizing the importance of solving real problems rather than just performing well on benchmarks. In the context of my research, this means that generated data samples not only adhere to theoretical expectations but also prove useful and effective in practical applications. This shift in focus has guided my research to prioritize real-world applicability. Verification becomes a step along the way to achieving robust validation. This is crucial because many research efforts emphasize verification, achieving high scores on benchmarks without considering whether these benchmarks accurately represent real-world scenarios.

Another key principle is Quality Assurance (QA) and Testing. As a developer, I understand the importance of having a robust QA process, which includes practices like code reviews, static analysis, and other quality of life improvements. However, working largely on my own presents unique challenges. Many QA techniques are time-consuming and require collaboration, such as code reviews, which aren't feasible in a solo PhD environment.

There is a clear discrepancy between the best practices in software development and the realities of pursuing a PhD. While a PhD should emphasize individual research and self-reliance, modern software development, particularly with agile methodologies, emphasizes teamwork and collective effort. This team-based approach is not always applicable to PhD students who often work independently.


## 3 Guest Lectures
#### Behavioral Software Engineering
Per Lenberg offered some fascinating insights into the importance of behavior in software development, emphasizing how corporate structures and relationships play crucial roles. One key takeaway is that hierarchies are not always as structured as we might assume, and human behavior is often irrational. I kind of assumed this was the case only for my wife, but it seems like it's widespread across society.

Although I am not sure if I can directly apply these insights to my research, understanding these dynamics can be beneficial. Recognizing the hidden hierarchies within corporate structures can be more important for achieving desired outcomes than relying solely on structured, rational approaches. While I do not particularly enjoy politics, it is essential to acknowledge that politics exist everywhere. Understanding and navigating these dynamics is crucial to avoid unnecessary friction. 

#### Volov's how to thow AI at a problems without thinking

Dhasarathy Parthasarathy offered a unique perspective on AI development. In stark contrast to Per Lenberg's lecture, Parthasarathy highlighted the pitfalls of technology-driven problem-solving without proper motivation or consideration of consequences. He forgot to emphasized the importance of questioning whether the latest technology is necessary, if it addresses the right problem, and if it is the appropriate tool for the job. Personally, I find it perplexing to use AI to verify a system, as it introduces another layer of complexity. How do you verify and validate the AI system that is supposed to verify your original system?

Parthasarathy's example addressed a common problem: ambiguities in the structure of enums across multiple services. He proposed using large language models (LLMs) to solve this issue by letting the LLM resolve ambiguities and write the tests. However, I believe a better approach would be to identify the ambiguities and resolve them at the architectural level. This approach would remove uncertainty and noise from the testing framework rather than adding to it. Without clear strategies for validation, verification, and maintenance, the proposed solution seems inadequate.

This perspective reinforces the idea from Lenberg's presentation that people are not always rational and that politics can overshadow responsibility. While testing may not be the most enjoyable activity, it is essential for verifying systems and ensuring they work as expected. Parthasarathy's insights remind us to prioritize responsibility and thorough testing over jumping on the AI bandwagon without considering the consequences.

## 4 Papers from CAIN

It feels like most of the papers presented at CAIN fall into two categories: wide empirical case studies that describe the challenges we are facing, the methodologies, steps, or design patterns used to address these challenges, and the uncertainty in solving these problems; and papers that focus on a single problem and propose a solution. To explore both approaches, I decided to examine one paper from each category.

#### Wide Empirical Case Study "A Meta-Summary of Challenges in Building Products with ML Components â€“ Collecting Experiences from 4758+ Practitioners": 

The core idea of this paper is to conduct a meta-summary of challenges faced by industry practitioners in building software products with machine learning (ML) components. The authors reviewed 50 studies that collectively interacted with over 4758 practitioners to identify common challenges across various stages of ML product development. This paper is crucial for AI systems engineering for several reasons. Firstly, it provides a comprehensive overview of the challenges faced by practitioners, which is essential for understanding the gaps and areas needing improvement in AI systems engineering. Secondly, by highlighting commonly reported challenges, the paper serves as a valuable resource for researchers and educators to prioritize efforts that address real-world issues encountered in the industry. Lastly, understanding these challenges informs the development of better engineering practices, tools, and methodologies to support the lifecycle of ML products, from requirements engineering to deployment and monitoring.

This information is not directly applicable to my current research, which is not focused on product development. Concepts like pipelining, testing, and verification are indeed relevant to my research because they streamline the process, I could repeat why it's relevant but I will spare you the time and you refer you to the earlier parts. However, I find it challenging to extract actionable insights on how to change my research based on this paper. While the knowledge is valuable for future endeavors, it does not have immediate applicability to my current work.

#### Problem-Specific Solution: "Towards Code Generation from BDD Test Case Specifications: A Vision"

The core idea of this paper is to propose a novel approach to generating frontend component code for the Angular framework using behavior-driven development (BDD) test specifications as input to a transformer-based machine learning model. This approach aims to drastically reduce the development time needed for web applications while potentially increasing software quality and introducing new research ideas toward automatic code generation.

This paper is important for AI systems engineering because it introduces an innovative method that leverages machine learning and natural language processing (NLP) to automate code generation from BDD test cases, significantly streamlining the software development process. By automating the generation of frontend components, developers can focus on higher-level tasks, improving efficiency and ensuring that the generated code adheres to predefined specifications, thus enhancing overall software quality. Furthermore, the proposed method incentivizes the use of meaningful test cases, encouraging developers to adopt robust quality assurance practices integral to high-quality software development.

This aligns well with my belief that humans should focus on verification and validation while leveraging technologies to automate as many daunting tasks as possible. It resonates with my research focus on validation-driven techniques and how they are leveraged to achieve good results. The approach of using behavior-driven development as prompts is particularly interesting. It seems applicable to my research, especially regarding pipelining and other aspects that are easy to write tests for. Although this research applies to front-end components, I believe the methodology could be generalized to other repetitive parts of software development, which are also present in my research.