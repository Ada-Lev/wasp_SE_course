\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
% More defined colors
\usepackage[dvipsnames]{xcolor}
 
% Required package
\usepackage{tikz}
\usetikzlibrary{positioning, shapes.symbols, arrows, shapes}
%%%%% ADDED BY TOMMY %%%%%
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{hyperref}
\usepackage{placeins}
% \usepackage{subfigure}
 
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newcommand{\Ie}{\textit{I.e.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\Eg}{\textit{E.g.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\probP}{\text{I\kern-0.15em P}}

\DeclareMathOperator*{\minimise}{\textrm{minimise}}
\DeclareMathOperator*{\maximise}{\textrm{maximise}}

\newcommand{\tl}[1]{\textit{[{\color{red}#1}]}}
\newcommand{\cm}[1]{\textit{{\color{blue}#1}}}
\newcommand{\erik}[1]{\textit{[{\color{brown}Erik: #1}]}}
\newcommand{\mr}[1]{\textit{[{\color{brown}Mohammed: #1}]}}
\renewcommand{\baselinestretch}{1.3} 

\title{A Systematic Approach to Robustness Modelling} 
% \title{Deckard: A Cloud-Native Platform for Robust Machine Learning }

\author[1]{Charles Meyers}
\affil[1]{Department of Computing Science, Umeå University, Umeå, Sweden}
% \title{Cloud, AI, and Robustness}
\date{May 2023}

\begin{document}
\maketitle
\begin{abstract}
    Considering the growing prominence of production-level AI and the threat of adversarial attacks that can poison a machine learning model against a certain label, evade classification, or reveal sensitive data about the model and training data to an attacker, adversaries pose fundamental problems to machine learning systems. Furthermore, much research has focused on the inverse relationship between robustness and accuracy, raising problems for real-time and safety-critical systems particularly since they are governed by legal constraints in which software changes must be explainable and every change must be thoroughly tested. While many defenses have been proposed, they are often computationally expensive and tend to reduce model accuracy. I have been evaluating a large suite of attacks and defenses and developing a framework for analyzing any machine-learning system from a safety-critical perspective using adversarial noise to find the upper bound of the failure rate. By developing systematic approaches to robustness testing, I hope to develop real-time tests that guarantee performance beyond the current test/train split paradigm.
\end{abstract}


\section{On the use Adversarial Attacks for AI Pipeline Verification }

Artificial Intelligence (AI) pipelines are often long-running and complex software tool-chains with many tunable hyperparameters. Managing, tracking, and controlling for various parameters is non trivial, but many management tools are available~\cite{dvc, hydra, k8s}. In general, a dataset is split into \textbf{training} and \textbf{test} sets. The training set is then used to determine the best configuration of a given model architecture on a given hardware architecture with the expectation that it will generalize both on the withheld test set and on new data generated by users via application programming interface (API) calls. To verify the training process, the test set validated against the \textit{inference} configuration of a model which may run on different hardware than the \textbf{training} configuration to reduce cost, latency, or power consumption.

\subsection{Adversarial Attacks}

In the context of machine learning, an adversarial attack refers to deliberate and malicious attempts to manipulate or exploit machine learning models. Adversarial attacks are designed to deceive or mislead the model's behavior by introducing carefully crafted input data that can cause the model to make incorrect predictions or produce undesired outputs.

The goal of an adversarial attack is often to exploit vulnerabilities in the model's decision-making process or to probe its weaknesses. These attacks can occur during various stages of the machine learning pipeline, including during training, inference, or deployment.

\label{attacks}
\begin{itemize}
    \item Evasion Attacks: These attacks aim to manipulate input data during the inference phase to deceive the model into misclassifying or ignoring certain inputs. Attackers carefully craft perturbations or modify the input features to mislead the model while still appearing similar to the original input \cite{biggio_evasion_2013, carlini_towards_2017, adversarialpatch, pixelattack, hopskipjump}.
    \item Poisoning Attacks: In poisoning attacks, the attacker intentionally injects malicious or manipulated training samples into the training dataset. The goal is to influence the model's behavior during training so that it learns to make incorrect predictions or exhibit unwanted behaviors when presented with specific inputs \cite{biggio_poisoning_2013, saha2020hidden}.
    \item Inference Attacks: These attacks exploit the model's output or responses to obtain sensitive information about the training data or other confidential details. By observing the model's predictions or confidence scores for carefully crafted inputs, attackers can extract information that should ideally be kept private \cite{chakraborty_adversarial_2018, orekondy2019knockoff}.
    \item Model Inversion Attacks: Model inversion attacks aim to infer sensitive information about the training data or proprietary model by exploiting the model's outputs. Attackers utilize the model's responses to iteratively reconstruct or approximate training examples that are similar to the ones used during training \cite{chakraborty_adversarial_2018, choquette2021label, li2021membership}.
\end{itemize}

\subsection{Security}
By using various types of attacks, we can model the security of AI pipelines as well. In order to measure robustness of a given model, it is assumed that the attacker knows most things about the model, including the distribution, shape, and feature space of the training set; the type of model used and it's parameter space; the gradients with respect to the optimization criteria; and feedback from the model in the form of model probability output. While it may seem like a prohibitively large set of assumptions, I outline possible attack vectors below. In our case, the attacker queries the model with an adversarial sample and is given the $\ell_{\infty}$ norm which returns the largest deviation of a single feature for a given sample rather than the more granular information provided by other standard distance metrics, like the $\ell_2$ or $\ell_{0}$ norms. It also ensures that no single feature is perturbed by more than $d_{max}$. 

\paragraph{Perfect Knowledge: }
While assumed the adversary has access to the model gradients with respect to the loss function, it can be approximated through Monte Carlo methods or via other attacks~\cite{wang2019security, chakraborty2018adversarial}. It is not necessary to know all of the model parameters, just the weights and biases that compose the fitted model. Although even this constraint is broken by other attacks~\cite{wang2019security, chakraborty2018adversarial}. The adversary can transform sample data, but must remain within a maximum distance $d_{max}$ for each feature. Other works~\cite{biggio2012poisoning, stutz2019confidence, li2016general} try to minimize the requisite perturbation distance. In others cases, perfect knowledge is provided normally by the peer-review process and published model weights. However, many models are proprietary and can only be accessed through an API that returns only the classification, either as a probability distribution or the $argmax$ of that distribution. Attacks with perfect knowledge are considered to be \textbf{black-box attacks} \cite{chakraborty_adversarial_2018}.


\subsection{Privacy }
\label{privacy}
Even though our attack scenario only includes perfect knowledge, prior research~\cite{fredrikson2015model, biggio2013evasion, chakraborty2018adversarial, wang2019security, ateniese2015hacking} has shown that a surrogate model and data-set can be used to approximate $f(x)$ by $\hat{f}(x)$ and build a model using the class labels provided by the model at test-time. Tram\`er et al.~\cite{tramer2016stealing} examined popular machine learning as a service platforms that return confidence values as well as class labels, showing that an attacker can build a proxy model by querying $ p + 1$ random $p-$dimensional inputs for unknown $p+1$ parameters. Further research~\cite{fredrikson2015model} were able to reverse engineer the training data-set through black-box attacks against a model that returns confidence levels, with the caveat that the inferred data might be a meta-prototypical example that does not appear in the original data-set. Fortunately for our attacker, such examples are still useful for determining the underlying data distributions even if they manage to preserve some of the privacy of the original data-set. Shokri et al.~\cite{shokri2017membership} presented a membership inference attack that determines whether a given data point belongs to the same distribution as the original training data using a set of proxy models. There are myriad ways for an attacker to get access to otherwise private data using nothing but standard machine learning APIs. Attacks that only require access to these APIs are considered \textbf{white-box attacks}.

\subsection{Safety}
The ISO standards \cite{iso26262} define the Safety Inegrity Level (SIL) in failures/per hour, which I have converted to failures per second in Table~\ref{tab:rate}. If assume that \textit{accidental} adversarial errors are possible in real-world systems due to things like dust, lens flare, component failure, packet loss, \textit{etc.}, it naturally follows that the adversarial failure rate is an estimate of the models behavior at the edge or in the `worst-case scenario'. That is, the \textit{adversarial failure rate} is an estimate of the upper bound of the real-world failure rate in adverse but otherwise mundane circumstances. However, due to the large number of samples required by regulatory standards  and the strenuous testing requirements of safety-critical software these evaluations become an infeasible way to verify that a model only fails once across the required number of samples (see Table~\ref{tab:rate}), especially if we would like to be highly confident of that estimation.

\begin{table}[!ht]
    \begin{center}
        \caption{Acceptable Failure Rates for different SIL levels in which a single death is possible, measured in failures per second.}
        \begin{tabular}{l c cc}
            \toprule
            SIL  && On-demand Operation   & Continuous Operation \\
            \cmidrule{1-1} \cmidrule{3-4}
            I     && $[10^{-6}, 10^{-5})$  & $[10^{-10}, 10^{-9})$ \\
            II    && $[10^{-7}, 10^{-6})$  & $[10^{-11}, 10^{-10})$ \\
            III   && $[10^{-8}, 10^{-7})$  & $[10^{-12}, 10^{-11})$ \\
            IV    && $[10^{-9}, 10^{-8})$  & $[10^{-13}, 10^{-12})$ \\
            \bottomrule
        \end{tabular}
        \label{tab:rate}
    \end{center}
\end{table} 


\subsection{Robustness}
\textit{Robustness}, then, is a measure of how well a model resists these \textit{induced} failures. For example, we could see how different AI-pipelines influence the accuracy given model architecture and dataset using the \textit{Percent Change in Accuracy} ($\%\Delta ACC$):

\begin{equation}
    \label{eq:percent_change_acc}
    \mathrm{\%\Delta ACC = 
        \frac{Acc.-Control~Acc}{Control~Acc}} \cdot 100
\end{equation}

where $Acc$ refers to the \textit{accuracy} and $Control$ refers to the performance of the unchanged model on the benign dataset. This measures the marginal risk of failure for a particular model change (defense) in the adversarial case when compared to the benign case. We could extend this to any other success metric like loss, the number of queries it takes to steal a database entry, or the time it takes to steal a model (see: Section~\ref{privacy} for more examples). In general \cite{meyers_1}, we can examine \textit{Relative Change in Failure Rate ($ \Delta \eta$):}

\begin{equation}
\label{eq:relative_change_failure_rate}
\mathrm{\Delta \eta} = 
    \frac{\mathrm{\eta_{control}-~\eta}}{\mathrm{\eta}}
\end{equation}
where $\eta$ refers to the failure rate, $Control$ refers to the unchanged model. Taken together, these two metrics allow us to measure the marginal risk of a given defense in both the benign and adversarial circumstances. In both cases, a positive number indicates an improvement in relative risk and a negative number indicates a worsening of relative risk, Eq.~\ref{eq:percent_change_acc} in the context of accuracy and Eq.~\ref{eq:relative_change_failure_rate} in the context of failure rate.

\section{Adversarial Attacks for CI/CD}
Historically, marginal gains in model performance have relied on exponentially larger models to produce increasingly marginal gains~\cite{desislavov2021compute}.  These models rely on increasingly larger datasets~\cite{desislavov2021compute, vapnik1994measuring, blumer1989learnability}, which increasingly come from fewer sources~\cite{koch2021reduced}, leading to gender-biased models~\cite{lu2020gender}, racism~\cite{buolamwini2018gender}, and fatal design errors~\cite{banks2018driver} This is a trend that goes back decades~\cite{corsaro1982something, ramirez2000resource, buolamwini2018gender}, leading to, for example, significantly higher fatality in car accidents for female-bodied people~\cite{evans2001gender} or neural networks that unintentionally encode racial information from medical imaging data alone~\cite{gichoya2022ai}. Furthermore, data collection can be expensive~\cite{roh2019survey}, raises serious privacy concerns~\cite{bloom2017self}, increases time to market~\cite{lam2004new}, and impedes development speed~\cite{zirger1996effect}. Furthermore, research is focused on metrics that tend to be optimistic at best~\cite{madry2017towards}. As noted by many researchers \cite{madry2017towards, carlini_towards_2017, croce_reliable_2020, meyers_1}, test/train split optimization can only find failures from `in-distribution` data. Instead, we need strong guarantees that our models will not fail subject to the aforementioned standards. It is also critical that these test run fast enough that they can be incorporated into the model-development feedback loop and keep, for example, untested AI software from needing an actual car to do verification test. Failure rate analysis has been widely explored in other fields \cite{aft_models}, but there's very little published research in the context of machine learning models.

\subsection{Quality Assurance}

The exponential distribution is a probability distribution that models the time until an event occurs in a continuous-time setting. In the context of failure rate, the exponential distribution is often used to characterize the failure or survival time of a system or component. The probability distribution function is given by:
\begin{align*}
& f(x) = \begin{cases}
\eta e^{-\eta x}, & x \geq 0 \\
0, & x < 0
\end{cases}\\
\end{align*}



The mean or expected value of the exponential distribution is given by:

\begin{align*}
& \mu = \frac{1}{\eta}\\
\end{align*}

The variance of the exponential distribution is given by:

\begin{align*}
& \sigma^2 = \frac{1}{\eta^2}
\end{align*}
This distribution is often used to model the case where a failure rate is invariant across samples \cite{aft_models}. However, we can also measure the effects of covariates and do this in an accelerated way if we are clever enough.


\section{Accelerated Failure Rate Models}


Accelerated failure rate models are statistical models used to analyze multivariate effects on the observed failure rate and to generate a model that's used to predict the failure rate across a wide variety of circumstances. The advantage of using Weibull, Log-Logistic, or Log-Normal distributions over methods like Kaplan-Maier \cite{aft_models} is that the latter is non-parametric, meaning that the resulting model does little to explain the model's performance across the various configurations. Additionally, semi-parametric models like the Cox Proportional Hazard model assume that the failure rate is constant over the covariates (usually time). Subsequently, this would do a poor job of modelling the effect of an attack, defence, or model configuration that changes the run-time or the prediction accuracy relative to the control. Furthermore, there is substantial empirical evidence \cite{meyers} that the efficacy of at least some attacks increases as we change various attack parameters. Therefore, in order to generate an explainable model that encapsulates the effect of the covariates,  we can use a distribution like Weibull, Log-Logistic, or Log-Normal \cite{aft_models}.

\subsection{Optimization}

Because of the relatively small run-time requirements of this approach (when compared to testing against massive in-distribution test sets), this method could, for example, act as a unit test in machine learning applications rather than relying on full-system integration tests to evaluate changes to a single model, signal processing technique, data storage format, or API access mechanism. It could also be used to highlight error-prone classes or other subsets of data to reduce error or create synthetic samples. Furthermore, by isolating changes and testing them as quickly as possible, it's much easier to parse cause and effect when compared to full-system integration tests that could include many changes from many different development teams and require live and potentially dangerous systems (like cars or MRI machines) to effectively test. To further increase development velocity,  metrics  Eq.~\ref{eq:percent_change_acc} and Eq.~\ref{eq:relative_change_failure_rate} have been proposed \cite{meyers_1} as standards for evaluating not only the efficacy of a given change, but as tools to quantify the marginal risk associated with each change, as dictated by the ISO 26262 standard \cite{iso26262}.

\section{Future Trends}

As stated by many researchers \cite{madry2017towards, carlini_towards_2017, croce_reliable_2020, meyers_1}, there is a reliability crisis effecting Artificial Intelligence. A very recent paper \cite{samuel2023computational} recently evaluated around 20 thousand jupyter notebooks that have been cited in medical studies. However, they found that fewer that 800 of them could run due to dependency issues. Interestingly, they also show how this trend has been improving over time. Whether that's due to time or an actually improved code-base is left to future research. 

\subsection{Lower-power Hardware}
Since the continued existence and efficacy of these attacks raises questions about the ability of these architectures to generalize since things like bit-depth \cite{feature_squeezing}, training-noise \cite{gauss_aug}, label-noise \cite{gauss_out}, and image resolution \cite{meyers} greatly vary the failure rate. In the real-world, effective resolution will change between individuals (e.g. a medical scan) or while moving (e.g. an autonomous vehicle). Even random noise drawn from approximately the same distribution as the training set\cite{gauss_aug, gauss_out} increases the benign failure rate by an order of magnitude or two. However, some research \cite{meyers_1} suggests that using low-bit-depth hardware could lower power requirements while \textit{increasing robustness}, though proving this is left to future research.

\subsection{Edge computing and Federation}
Edge computing \cite{li2019edge, deng2020edge} inverts the normal paradigm for datacenters. Instead of having on-demand, always available, high-bandwidth, and redundant systems, we rely on faulty, unreliable, low-power systems with variable bandwidth on a distributed network. The benefits for AI applications could be immense. Drones and autonomous vehicles could offload their processing, content distribution networks can move from giant, remote datacenters to a bandwidth-minimizing swarm dotted around the neighborhood. Medical care could be cheaper, faster, and more accurate. However, we have a long way to go. Fortunately, model federation \cite{li2020review} solves many of these problems. It could solve privacy concerns for many applications \cite{shamimmachine, bhagoji2019analyzing}. 

\large 
\bibliographystyle{IEEEtran}
\bibliography{bibliography}
\end{document}
